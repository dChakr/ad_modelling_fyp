{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Composite Cognitive Score \n",
    "Predict the composite cognitive score of a set of ADNI patients using Random Forrest and SVM methods. \n",
    "\n",
    "We are using the four ADSP-PHC composite scores for *Memory, Executive Function, Language and Visuospatial Ability*. The methods for deriving these are described in 'ADSP Phenotype Harmonization Consortium â€“ Derivation of Cognitive Composite Scores' by Mukherjee et al (https://ida.loni.usc.edu/download/files/study/083f5b49-98d1-494a-aaf1-3310a9a8e62c/file/adni/ADNI_Cognition_Methods_Psychometric_Analyses_Oct2022.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# scikit-learn modules\n",
    "from sklearn.model_selection import train_test_split # for splitting the data\n",
    "from sklearn.ensemble import RandomForestRegressor # for building the model\n",
    "\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data\n",
    "\n",
    "Match up the composite cognitive scores and functional connectivity data, then split into test + training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADSP_DATA_PATH = \"data/ADSP_PHC_COGN_Dec2023_FILTERED.csv\"\n",
    "FC_DATA_PATH = \"../FMRI_ADNI_DATA/fc/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the ADSP Data\n",
    "\n",
    "adsp_df = pd.read_csv(ADSP_DATA_PATH)\n",
    "adsp_df = adsp_df.drop(columns=adsp_df.columns[0])\n",
    "adsp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adsp_df = adsp_df.drop(columns=[\n",
    "    'SUBJID', 'PHASE', 'VISCODE', 'EXAMDATE', 'PHC_Visit', 'PHC_Sex', 'PHC_Education', 'PHC_Ethnicity', 'PHC_Race', 'PHC_Age_Cognition', \n",
    "    'PHC_MEM_SE', 'PHC_MEM_PreciseFilter', 'PHC_EXF_SE', 'PHC_EXF_PreciseFilter', 'PHC_LAN_SE', 'PHC_LAN_PreciseFilter', 'PHC_VSP_SE',\n",
    "    'PHC_VSP_PreciseFilter'\n",
    "])\n",
    "adsp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_viscode(str):\n",
    "    if str == 'BL' or str == 'SC':\n",
    "        return adsp_df['VISCODE2'].replace(str, 'M000')\n",
    "    else:\n",
    "        vis = str[1:]\n",
    "        vis = vis.zfill(3)\n",
    "        vis = 'M' + vis\n",
    "        return adsp_df['VISCODE2'].replace(str, vis)\n",
    "\n",
    "adsp_df['VISCODE2'] = adsp_df['VISCODE2'].str.upper()\n",
    "\n",
    "# Pad the visit codes\n",
    "for val in adsp_df['VISCODE2'].unique():\n",
    "    adsp_df['VISCODE2'] = replace_viscode(val)\n",
    "\n",
    "# Pad the RID values\n",
    "adsp_df['RID'] = adsp_df['RID'].apply(lambda x: str(x).zfill(4))\n",
    "\n",
    "adsp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the FC data and add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_rid_viscode(filename):\n",
    "    pattern = r'sub-ADNI\\d+S(\\d{4})_ses-(M\\d{3})'\n",
    "    match = re.search(pattern, filename)\n",
    "\n",
    "    if match:\n",
    "        rid = match.group(1)\n",
    "        viscode = match.group(2)\n",
    "        return rid, viscode        \n",
    "    else:\n",
    "        print(\"Pattern not found in the filename.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adsp_df['FC_DATA'] = None\n",
    "\n",
    "fc_dir = os.listdir(FC_DATA_PATH)\n",
    "\n",
    "fc_files = [os.path.join(FC_DATA_PATH, file) for file in fc_dir if file.endswith('.mat')]\n",
    "len(fc_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adsp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fc in fc_files:\n",
    "    rid, viscode = get_rid_viscode(fc)\n",
    "    adsp_df.loc[(adsp_df['RID'] == rid) & (adsp_df['VISCODE2'] == viscode), 'FC_DATA'] = fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adsp_df_filtered = adsp_df[adsp_df['FC_DATA'].notna()]\n",
    "adsp_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adsp_df_filtered = adsp_df_filtered.drop(adsp_df_filtered[adsp_df_filtered['VISCODE2'] == 'M162'].index)\n",
    "adsp_df_filtered = adsp_df_filtered.drop(adsp_df_filtered[adsp_df_filtered['VISCODE2'] == 'M174'].index)\n",
    "adsp_df_filtered = adsp_df_filtered.drop(adsp_df_filtered[adsp_df_filtered['VISCODE2'] == 'M180'].index)\n",
    "adsp_df_filtered = adsp_df_filtered.drop(adsp_df_filtered[adsp_df_filtered['VISCODE2'] == 'M186'].index)\n",
    "adsp_df_filtered = adsp_df_filtered.drop(adsp_df_filtered[adsp_df_filtered['VISCODE2'] == 'M192'].index)\n",
    "adsp_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the adsp_df_filtered dataframe as a file\n",
    "# adsp_df_filtered.to_csv('data/ADSP_PHC_COGN_Dec2023_FILTERED_wfiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the FC data as numpy arrays\n",
    "dim_x = len(adsp_df_filtered['FC_DATA'])\n",
    "features = np.zeros(shape=(dim_x, 100, 200)) # get the first 100 regions\n",
    "\n",
    "for i, file in enumerate(adsp_df_filtered['FC_DATA'].values):\n",
    "    arr = loadmat(file)['ROI_activity'][:100, :] # get the first 100 regions\n",
    "    if arr.shape[1] != 200:\n",
    "        # add padding to get a constant shape\n",
    "        diff = 200 - arr.shape[1]\n",
    "        if diff < 0:\n",
    "            arr = arr[:, :200]\n",
    "        else:\n",
    "            pad_width = ((0, 0), (0, diff))  \n",
    "            padded_array = np.pad(arr, pad_width, mode='constant', constant_values=0)\n",
    "    features[i] = padded_array\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = adsp_df_filtered[['PHC_MEM', 'PHC_EXF', 'PHC_LAN', 'PHC_VSP']]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into test + training (80% train, 20% test)\n",
    "features_2d = features.reshape(features.shape[0], -1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(features_2d, y, test_size = 0.2, random_state = 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest Method\n",
    "Prediction not differentiable wrt to input - need a model per composite (memory, executive function, language and visuospatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split targets into the different composites\n",
    "\n",
    "# y_train_mem, y_train_exf, y_train_lan, y_train_vsp = y_train['PHC_MEM'], y_train['PHC_EXF'], y_train['PHC_LAN'], y_train['PHC_VSP']\n",
    "# y_test_mem, y_test_exf, y_test_lan, y_test_vsp = y_test['PHC_MEM'], y_test['PHC_EXF'], y_test['PHC_LAN'], y_test['PHC_VSP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEMORY MODEL\n",
    "\n",
    "# Remove NaNs in target\n",
    "y_train_mem = y_train['PHC_MEM'].reset_index(drop=True)\n",
    "y_test_mem =y_test['PHC_MEM'].reset_index(drop=True)\n",
    "\n",
    "nan_indices = y_train_mem.index[y_train_mem.isna()]\n",
    "y_train_mem = y_train_mem.drop(nan_indices)\n",
    "x_train_mem = np.delete(x_train, nan_indices, axis = 0)\n",
    "# print(nan_indices)\n",
    "\n",
    "nan_indices_test = y_test_mem.index[y_test_mem.isna()]\n",
    "y_test_mem = y_test_mem.drop(nan_indices_test)\n",
    "x_test_mem = np.delete(x_test, nan_indices_test, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Random Forest Regression model with 10 decision trees\n",
    "base_model_mem = RandomForestRegressor(n_estimators = 10, random_state = 5)\n",
    "\n",
    "# Fitting the Random Forest Regression model to the data\n",
    "base_model_mem.fit(x_train_mem, y_train_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test_mem.shape\n",
    "# Predicting the target values of the test set\n",
    "base_y_pred_mem = base_model_mem.predict(x_test_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "base_r2_mem = r2_score(y_test_mem, base_y_pred_mem)\n",
    "print(\"Baseline R2 (MEM): \", base_r2_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_model_mem.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search for best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTOR_TYPE = 'EXF'\n",
    "\n",
    "y_train_cleaned = y_train[f'PHC_{PREDICTOR_TYPE}'].reset_index(drop=True)\n",
    "y_test_cleaned =y_test[f'PHC_{PREDICTOR_TYPE}'].reset_index(drop=True)\n",
    "\n",
    "nan_indices = y_train_cleaned.index[y_train_cleaned.isna()]\n",
    "y_train_cleaned = y_train_cleaned.drop(nan_indices)\n",
    "x_train_cleaned = np.delete(x_train, nan_indices, axis = 0)\n",
    "\n",
    "nan_indices_test = y_test_cleaned.index[y_test_cleaned.isna()]\n",
    "y_test_cleaned = y_test_cleaned.drop(nan_indices_test)\n",
    "x_test_cleaned = np.delete(x_test, nan_indices_test, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [100, 119, 138, 157, 176, 195, 215, 234, 253, 272, 291, 311, 330, 349, 368, 387, 407, 426, 445, 464, 483, 503, 522, 541, 560, 579, 598, 618, 637, 656, 675, 694, 714, 733, 752, 771, 790, 810, 829, 848, 867, 886, 906, 925, 944, 963, 982, 1002, 1021, 1040, 1059, 1078, 1097, 1117, 1136, 1155, 1174, 1193, 1213, 1232, 1251, 1270, 1289, 1309, 1328, 1347, 1366, 1385, 1405, 1424, 1443, 1462, 1481, 1501, 1520, 1539, 1558, 1577, 1596, 1616, 1635, 1654, 1673, 1692, 1712, 1731, 1750, 1769, 1788, 1808, 1827, 1846, 1865, 1884, 1904, 1923, 1942, 1961, 1980, 2000], 'max_features': ['log2', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 100)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from joblib import dump\n",
    "\n",
    "for PREDICTOR_TYPE in ['MEM', 'EXF', 'LAN', 'VSP']:\n",
    "\n",
    "    # clean the data\n",
    "    y_train_cleaned = y_train[f'PHC_{PREDICTOR_TYPE}'].reset_index(drop=True)\n",
    "    y_test_cleaned =y_test[f'PHC_{PREDICTOR_TYPE}'].reset_index(drop=True)\n",
    "\n",
    "    nan_indices = y_train_cleaned.index[y_train_cleaned.isna()]\n",
    "    y_train_cleaned = y_train_cleaned.drop(nan_indices)\n",
    "    x_train_cleaned = np.delete(x_train, nan_indices, axis = 0)\n",
    "\n",
    "    nan_indices_test = y_test_cleaned.index[y_test_cleaned.isna()]\n",
    "    y_test_cleaned = y_test_cleaned.drop(nan_indices_test)\n",
    "    x_test_cleaned = np.delete(x_test, nan_indices_test, axis = 0)\n",
    "    # ==================================================================\n",
    "\n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    \n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestRegressor()\n",
    "\n",
    "    # Evaluation metric\n",
    "    r2_scorer = make_scorer(r2_score)\n",
    "\n",
    "    # Random search of parameters, using 5 fold cross validation, \n",
    "\n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_random = RandomizedSearchCV(\n",
    "        estimator = rf, param_distributions = random_grid, scoring=r2_scorer,\n",
    "        n_iter = 100, cv = 5, verbose=4, random_state=42, n_jobs = -1)\n",
    "\n",
    "    # Fit the random search model\n",
    "    rf_random.fit(x_train_mem, y_train_mem)\n",
    "    \n",
    "    # ======================================================================\n",
    "    \n",
    "    # Get the best parameter set\n",
    "    best_params = rf_random.best_params_\n",
    "    PARAM_FILE = f'{PREDICTOR_TYPE}_best_params.json'\n",
    "\n",
    "    # Write data to a JSON file\n",
    "    with open(PARAM_FILE, 'w') as json_file:\n",
    "        json.dump(best_params, json_file)\n",
    "\n",
    "    print(\"\\n The best estimator across ALL searched params:\\n\", rf_random.best_estimator_)\n",
    "    print(\"\\n The best score across ALL searched params:\\n\", rf_random.best_score_)\n",
    "    print(\"\\n The best parameters across ALL searched params:\\n\", rf_random.best_params_)\n",
    "\n",
    "    # save the model\n",
    "    dump(rf_random.best_estimator_, f'best_model_{PREDICTOR_TYPE}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from joblib import dump\n",
    "\n",
    "# Get the best parameter set\n",
    "best_params = rf_random.best_params_\n",
    "PARAM_FILE = f'{PREDICTOR_TYPE}_best_params.json'\n",
    "\n",
    "# Write data to a JSON file\n",
    "with open(PARAM_FILE, 'w') as json_file:\n",
    "    json.dump(best_params, json_file)\n",
    "    \n",
    "print(\"\\n The best estimator across ALL searched params:\\n\", rf_random.best_estimator_)\n",
    "print(\"\\n The best score across ALL searched params:\\n\", rf_random.best_score_)\n",
    "print(\"\\n The best parameters across ALL searched params:\\n\", rf_random.best_params_)\n",
    "\n",
    "# save the model\n",
    "dump(rf_random.best_estimator_, f'best_model_{PREDICTOR_TYPE}.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.11 (test)",
   "language": "python",
   "name": "python311_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
