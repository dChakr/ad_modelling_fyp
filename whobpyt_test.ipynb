{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'whobpyt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwhobpyt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwhobpyt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatatypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m par, Recording\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwhobpyt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mRWW\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RNNRWW, ParamsRWW\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'whobpyt'"
     ]
    }
   ],
   "source": [
    "import whobpyt\n",
    "from whobpyt.datatypes import par, Recording\n",
    "from whobpyt.models.RWW import RNNRWW, ParamsRWW\n",
    "from whobpyt.models.RWWEI2 import RWWEI2, ParamsRWWEI2\n",
    "from whobpyt.optimization.custom_cost_RWW import CostsRWW\n",
    "from whobpyt.run import Model_fitting\n",
    "\n",
    "import os\n",
    "import sys \n",
    "\n",
    "# array and pd stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat # for reading in the .mat files\n",
    "\n",
    "import torch\n",
    "\n",
    "# viz stuff\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is cuda avaliable?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Is cuda avaliable?\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cpu\") #Options: \"cpu\" or \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the FC data and find Correlation matrix\n",
    "def normalise_correlate_fc(fc):\n",
    "    fc_emp = fc / np.max(fc)\n",
    "    fc_emp = np.corrcoef(fc_emp.T)\n",
    "    return fc_emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_fc(fcs):\n",
    "    corr_matrices = []\n",
    "\n",
    "    for data in fcs:\n",
    "        fc = normalise_correlate_fc(data)\n",
    "        corr_matrices.append(fc)\n",
    "\n",
    "    stacked_corr = np.stack(corr_matrices, axis = 0)\n",
    "    avg_corr = np.mean(stacked_corr, axis =0)\n",
    "    \n",
    "    return avg_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# load raw data and get SC empirical BOLD and FC\n",
    "\n",
    "sc = np.genfromtxt('data/DTI_fiber_consensus_HCP.csv', delimiter=',')\n",
    "with open('data/healthy_subjs.txt', 'r') as file:\n",
    "    # Read all lines into a list\n",
    "    fc_files = file.readlines()\n",
    "\n",
    "fc_files = [file.strip() for file in fc_files]\n",
    "\n",
    "fcs = []\n",
    "\n",
    "for f in fc_files[:10]:\n",
    "#     print(f)\n",
    "    fc = loadmat(f)\n",
    "    fc = fc['ROI_activity'][:100, :]\n",
    "    fc = fc.T\n",
    "    # print(fc.shape)\n",
    "    fcs.append(fc)\n",
    "\n",
    "print(len(fcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define options for wong-wang model\n",
    "node_size = 100\n",
    "mask = np.tril_indices(node_size, -1)\n",
    "num_epochs = 5\n",
    "TPperWindow = 20\n",
    "step_size = 0.05\n",
    "input_size = 2\n",
    "tr = 0.75\n",
    "repeat_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "SC = (sc + sc.T) * 0.5\n",
    "sc = np.log1p(SC) / np.linalg.norm(np.log1p(SC))\n",
    "print(sc.shape)\n",
    "\n",
    "fc_emp = get_avg_fc(fcs)\n",
    "\n",
    "# normalise all fcs\n",
    "for fc in fcs:\n",
    "    fc = fc / np.max(fc)\n",
    "    \n",
    "# prepare data structure of the model\n",
    "fMRIstep = tr\n",
    "\n",
    "avgfc = Recording(fc_emp.T, fMRIstep)\n",
    "\n",
    "# for multiple empirical bolds:\n",
    "rcds = []\n",
    "\n",
    "for f in fcs:\n",
    "    rcds.append(Recording(f.T, fMRIstep))\n",
    "print(len(rcds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/dc420/home/ad_modelling_fyp/whobpyt/datatypes/parameter.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val = torch.tensor(val, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RWWEI2.__init__() missing 1 required positional argument: 'Dist_Mtx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m paramsNode\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# call model want to fit\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# model = RNNRWW(node_size, TPperWindow, step_size, repeat_size, tr, sc, True, params)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mRWWEI2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_regions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparamsNode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCon_Mtx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msim_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtr\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# create objective function\u001b[39;00m\n\u001b[1;32m     17\u001b[0m ObjFun \u001b[38;5;241m=\u001b[39m CostsRWW(model, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mTypeError\u001b[0m: RWWEI2.__init__() missing 1 required positional argument: 'Dist_Mtx'"
     ]
    }
   ],
   "source": [
    "# get model parameters structure and define the fitted parameters by setting non-zero variance for the model\n",
    "\n",
    "# params = ParamsRWWEI2(G=par(100, 100, 1/np.sqrt(10), True, True), g_EE=par(3.5, 3.5, 1/np.sqrt(50), True, True), g_EI =par(0.42, 0.42, 1/np.sqrt(50), True, True), \\\n",
    "#                    g_IE=par(0.42, 0.42, 1/np.sqrt(50), True, True), I_0 =par(0.2), std_in=par(0.0), std_out=par(0.00))\n",
    "# params.to(device)\n",
    "\n",
    "paramsNode = ParamsRWWEI2(num_regions=100)\n",
    "paramsNode.G = par(torch.tensor(2.0), fit_par = True, asLog = True)\n",
    "paramsNode.to(device)\n",
    "\n",
    "# call model want to fit\n",
    "# model = RNNRWW(node_size, TPperWindow, step_size, repeat_size, tr, sc, True, params)\n",
    "model = RWWEI2(num_regions=100, params=paramsNode, Con_Mtx=sc, step_size=step_size, sim_len=tr*1000, device = device)\n",
    "\n",
    "# %%\n",
    "# create objective function\n",
    "ObjFun = CostsRWW(model, device=device)\n",
    "\n",
    "# %%\n",
    "# call model fit\n",
    "F = Model_fitting(model, ObjFun, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "epoch:  0 loss: 8.616871 Pseudo FC_cor:  0.05864909609208868 cos_sim:  -0.9961187318648489\n",
      "epoch:  0 loss: 7.3086824 Pseudo FC_cor:  0.0025978414339729336 cos_sim:  -0.9462153362665393\n",
      "epoch:  0 loss: 6.200033 Pseudo FC_cor:  0.06882899599707246 cos_sim:  -0.8523910932426988\n",
      "epoch:  0 loss: 6.1356406 Pseudo FC_cor:  0.032581616316987784 cos_sim:  -0.995627969452897\n",
      "epoch:  0 loss: 5.0131907 Pseudo FC_cor:  0.036970052779766345 cos_sim:  -0.9980271132105848\n",
      "epoch:  0 loss: 5.508816 Pseudo FC_cor:  0.08180350799982895 cos_sim:  -0.9990860530006043\n",
      "epoch:  0 loss: 5.1826406 Pseudo FC_cor:  -0.004234801622686044 cos_sim:  0.6716482492087398\n",
      "epoch:  0 loss: 5.113417 Pseudo FC_cor:  0.08138770590581489 cos_sim:  -0.8804539205630416\n",
      "epoch:  0 loss: 4.7641306 Pseudo FC_cor:  0.04742515676753192 cos_sim:  -0.9958665093837051\n",
      "epoch:  0 loss: 4.466594 Pseudo FC_cor:  -0.007319240608141967 cos_sim:  -0.9981164314755796\n",
      "Epoch:  1\n",
      "epoch:  1 loss: 4.569216 Pseudo FC_cor:  0.1846213172702461 cos_sim:  -0.9991414783406691\n",
      "epoch:  1 loss: 4.652992 Pseudo FC_cor:  0.05225690858048577 cos_sim:  -0.9456590646782915\n",
      "epoch:  1 loss: 4.499461 Pseudo FC_cor:  0.038559712963538204 cos_sim:  -0.8516055495208615\n",
      "epoch:  1 loss: 4.237053 Pseudo FC_cor:  0.1531390638834087 cos_sim:  -0.9949920938424267\n",
      "epoch:  1 loss: 4.2755175 Pseudo FC_cor:  0.12081567890841094 cos_sim:  -0.9971778772250417\n",
      "epoch:  1 loss: 4.0807266 Pseudo FC_cor:  0.0491637656618241 cos_sim:  -0.9980234082781599\n",
      "epoch:  1 loss: 4.525553 Pseudo FC_cor:  0.04392638340564713 cos_sim:  0.6699784042013688\n",
      "epoch:  1 loss: 4.0502586 Pseudo FC_cor:  0.0510105004777117 cos_sim:  -0.8780396483987163\n",
      "epoch:  1 loss: 3.9488358 Pseudo FC_cor:  0.039615487275537216 cos_sim:  -0.9953013157506421\n",
      "epoch:  1 loss: 3.7486465 Pseudo FC_cor:  0.04235447138127335 cos_sim:  -0.9975331159647642\n",
      "Epoch:  2\n",
      "epoch:  2 loss: 4.3567357 Pseudo FC_cor:  0.11671128454260749 cos_sim:  -0.9982187041196834\n",
      "epoch:  2 loss: 3.6336703 Pseudo FC_cor:  0.07519589163453141 cos_sim:  -0.9453293071829576\n",
      "epoch:  2 loss: 4.161952 Pseudo FC_cor:  -0.014200643562675128 cos_sim:  -0.8512591316956265\n",
      "epoch:  2 loss: 3.1554701 Pseudo FC_cor:  0.17035396800153327 cos_sim:  -0.9951135284595034\n",
      "epoch:  2 loss: 2.8193865 Pseudo FC_cor:  0.006454232729351196 cos_sim:  -0.9976185448732835\n",
      "epoch:  2 loss: 3.3821125 Pseudo FC_cor:  0.04694082700028867 cos_sim:  -0.9982841102196996\n",
      "epoch:  2 loss: 3.6143587 Pseudo FC_cor:  0.06492478125625444 cos_sim:  0.6707948639145846\n",
      "epoch:  2 loss: 3.5542355 Pseudo FC_cor:  0.04406118579750967 cos_sim:  -0.8810514877856115\n",
      "epoch:  2 loss: 3.1696553 Pseudo FC_cor:  0.0720690589964585 cos_sim:  -0.9956846267973275\n",
      "epoch:  2 loss: 2.888484 Pseudo FC_cor:  0.037744076652868164 cos_sim:  -0.9974458162200844\n",
      "Epoch:  3\n",
      "epoch:  3 loss: 2.8396056 Pseudo FC_cor:  0.11582252265828602 cos_sim:  -0.998662828506143\n",
      "epoch:  3 loss: 3.0047495 Pseudo FC_cor:  0.1027753985085614 cos_sim:  -0.945667244837361\n",
      "epoch:  3 loss: 3.0681298 Pseudo FC_cor:  0.08010833911038144 cos_sim:  -0.8516613134906234\n",
      "epoch:  3 loss: 3.0593565 Pseudo FC_cor:  0.13007043143364888 cos_sim:  -0.9948792401714464\n",
      "epoch:  3 loss: 2.5896897 Pseudo FC_cor:  0.11360722818162751 cos_sim:  -0.9973233064239463\n",
      "epoch:  3 loss: 3.1595192 Pseudo FC_cor:  0.07455250167970902 cos_sim:  -0.9987243790332377\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Model Training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mempRecs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrcds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTPperWindow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTPperWindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearningrate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ad_modelling_fyp/whobpyt/run/modelfitting.py:183\u001b[0m, in \u001b[0;36mModel_fitting.train\u001b[0;34m(self, u, empRecs, num_epochs, TPperWindow, learningrate, lr_2ndLevel, lr_scheduler)\u001b[0m\n\u001b[1;32m    180\u001b[0m loss_his\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Calculate gradient using backward (backpropagation) method of the loss function.\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Optimize the model based on the gradient method in updating the model parameters.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m hyperparameter_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "# ---------------------------------------------------\n",
    "#\n",
    "F.train(u = 0, empRecs = rcds, num_epochs = num_epochs, TPperWindow = TPperWindow, learningrate = 0.05, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of loss over Training\n",
    "plt.plot(np.arange(1,len(F.trainingStats.loss)+1), F.trainingStats.loss, label='loss')\n",
    "plt.title(\"Total Loss over Training Epochs\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Plots of parameters values over Training\n",
    "plt.plot(F.trainingStats.fit_params['g_EE'], label = \"g_EE\")\n",
    "plt.plot(F.trainingStats.fit_params['g_EI'], label = \"g_EI\")\n",
    "plt.plot(F.trainingStats.fit_params['g_IE'], label = \"g_IE\")\n",
    "plt.legend()\n",
    "plt.title(\"Local Coupling Variables Changing Over Training Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.evaluate(u = 0, empRecs = rcds[0], TPperWindow = TPperWindow, base_window_num = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot E I and simulated BOLD\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 8))\n",
    "ax[0].plot(F.lastRec['E'].npTS().T)\n",
    "ax[0].set_title('Test: E')\n",
    "ax[1].plot(F.lastRec['I'].npTS().T)\n",
    "ax[1].set_title('Test: I')\n",
    "ax[2].plot(F.lastRec['bold'].npTS().T)\n",
    "ax[2].set_title('Test: BOLD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Plot the FC and the test FC\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 8))\n",
    "im0 = ax[0].imshow(fc_emp, cmap='bwr')\n",
    "ax[0].set_title('The empirical FC')\n",
    "fig.colorbar(im0, ax=ax[0], fraction=0.046, pad=0.04, label='Connection Weights')\n",
    "test = F.lastRec['bold'].npTS()\n",
    "\n",
    "fc_test = np.corrcoef(F.lastRec['bold'].npTS())\n",
    "im1 = ax[1].imshow(fc_test, cmap='bwr')\n",
    "ax[1].set_title('The simulated FC')\n",
    "fig.colorbar(im1, ax=ax[1], fraction=0.046, pad=0.04, label='Connection Weights')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
